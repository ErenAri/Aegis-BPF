# AegisBPF Performance Profile

Version: 1.0 (2026-02-09)

This document provides guidance on AegisBPF's runtime resource consumption,
expected overhead, and tuning parameters.

## CPU overhead per hook

AegisBPF attaches BPF programs to kernel LSM hooks.  Each hooked syscall
incurs additional latency from the BPF program execution.

### Expected overhead ranges

| Hook | Typical added latency | Notes |
|------|----------------------|-------|
| `file_open` / `inode_permission` | 0.1–0.5 us | O(1) hash lookup in `deny_inode_map` |
| `socket_connect` | 0.2–1.0 us | Hash lookup (exact IP) + LPM lookup (CIDR) + port check |
| `socket_bind` | 0.2–0.8 us | Port + protocol hash lookup |
| Tracepoint `sys_enter_openat` (audit fallback) | 0.1–0.3 us | Path string comparison only |

These numbers are for the BPF program execution itself.  Actual end-to-end
syscall overhead depends on kernel version, hardware, and system load.

### Factors that do NOT affect per-syscall overhead

- **Number of deny rules:** BPF map lookups are O(1) for hash maps and
  O(prefix-length) for LPM tries.  Adding more rules does not increase lookup
  time (until hash collision rates rise at very high entry counts).
- **Ring buffer size:** The ring buffer is written asynchronously.  Buffer size
  affects memory consumption, not per-syscall latency.

### Factors that DO affect per-syscall overhead

- **BPF JIT availability:** Kernels without JIT (`/proc/sys/net/core/bpf_jit_enable=0`)
  run BPF programs in an interpreter, which is significantly slower.
- **Kernel version:** Newer kernels have faster BPF infrastructure.
- **CPU architecture:** x86_64 with JIT is the primary benchmark target.

## Memory consumption

### Formula

```
total_memory ≈ bpf_maps + ring_buffer + per_cpu_arrays + userspace_heap
```

### BPF map memory

| Map | Key size | Value size | Default max_entries | Memory per entry |
|-----|----------|------------|--------------------|--------------------|
| `deny_inode_map` | 16 B | 1 B | 65536 | ~80 B (with BPF overhead) |
| `deny_path_map` | 256 B | 1 B | 65536 | ~320 B |
| `allow_cgroup_map` | 8 B | 1 B | 256 | ~72 B |
| `deny_ipv4_map` | 4 B | 1 B | 16384 | ~68 B |
| `deny_ipv6_map` | 16 B | 1 B | 16384 | ~80 B |
| `deny_port_map` | 4 B | 1 B | 4096 | ~68 B |
| `deny_cidr_v4_map` (LPM) | 8 B | 1 B | 16384 | ~100 B |
| `deny_cidr_v6_map` (LPM) | 20 B | 1 B | 16384 | ~110 B |
| `block_stats` | 4 B | 16 B | 1 | per-CPU array |
| `net_block_stats` | 4 B | 24 B | 1 | per-CPU array |
| `survival_allowlist` | 16 B | 1 B | 256 | ~80 B |

BPF hash map overhead is approximately 2-4x the raw key+value size due to
internal hash table structure, per-element metadata, and alignment.

**Approximate total for default configuration with empty maps:**
~5-10 MB (dominated by pre-allocated hash table buckets).

**With 10,000 inode deny rules:**
~5 MB base + 10,000 * 80 B ≈ 5.8 MB

### Ring buffer memory

The ring buffer is a shared memory region between kernel and userspace.

```
ring_buffer_memory = ringbuf_size_bytes  (default: 256 KB)
```

The ring buffer size must be a power of 2.  It can be tuned via
`--ringbuf-size=<bytes>` at agent startup.

### Per-CPU arrays

Stats maps (`block_stats`, `net_block_stats`, per-map stats) use per-CPU
arrays:

```
per_cpu_memory = num_stats_maps * value_size * num_cpus
```

On a 64-core host with all stats maps: ~64 * 7 * 24 B ≈ 10 KB.

### Userspace heap

The agent's userspace memory is dominated by:
- Policy parsing buffers (transient, freed after apply)
- Deny entry tracking (`DenyEntries` map, ~100 B per entry)
- Cgroup path cache (~500 B per cached entry)
- Event processing buffers (ring buffer consumer)

Typical steady-state userspace RSS: 5-15 MB.

## Ring buffer sizing guidance

The ring buffer holds events generated by BPF programs until the userspace
consumer drains them.

### Sizing formula

```
recommended_size = events_per_second * avg_event_size * buffer_seconds
```

| Event type | Approximate size |
|-----------|-----------------|
| `ExecEvent` | 40 B |
| `BlockEvent` | 336 B |
| `NetBlockEvent` | 104 B |

### Example

For a workload generating 1,000 block events/sec with 2 seconds of buffer:

```
1000 * 336 * 2 = 672,000 B → round up to 1 MB (next power of 2)
```

### Pressure behavior

When the ring buffer is full:
- **New events are dropped**, not queued.  The BPF program increments the
  `ringbuf_drops` counter in the `block_stats` map.
- **Enforcement is NOT affected** by ring buffer pressure.  Deny decisions
  happen before event emission.  A full ring buffer means lost telemetry, not
  lost enforcement.
- The agent logs a warning when `ringbuf_drops` increases between poll cycles.

### Monitoring ring buffer health

```bash
# Check current drop count
aegisbpf stats --detailed

# Prometheus metric
aegisbpf_ringbuf_drops_total
```

If drops are persistent, increase the ring buffer size:
```bash
aegisbpf run --ringbuf-size=1048576  # 1 MB
```

## Capacity planning

Use the `footprint` subcommand to estimate memory requirements before
deployment:

```bash
aegisbpf footprint \
    --deny-inodes=10000 \
    --deny-paths=5000 \
    --deny-ips=1000 \
    --deny-cidrs=500 \
    --deny-ports=100 \
    --ringbuf-bytes=524288
```

This outputs estimated memory for each map and the total.

## Benchmarking

### Userspace benchmarks

Userspace hot-path benchmarks (policy parsing, hashing, key construction) run
without root privileges:

```bash
./build/aegisbpf_bench --benchmark_format=json
```

### Syscall-level benchmarks

Syscall benchmarks measure actual `open()` and `connect()` latency with BPF
hooks attached.  These require root:

```bash
sudo scripts/bench_syscall.sh --json --out results.json
```

### Shell-based benchmarks

For quick A/B comparisons with and without the agent running:

```bash
# Baseline (no agent)
scripts/perf_open_bench.sh

# With agent
sudo WITH_AGENT=1 scripts/perf_open_bench.sh

# Network
sudo WITH_AGENT=1 scripts/perf_connect_bench.sh
```

## Tuning recommendations

| Parameter | Default | Guidance |
|-----------|---------|----------|
| `--ringbuf-size` | 256 KB | Increase if `ringbuf_drops` is non-zero |
| `--max-deny-inodes` | 65536 | Increase for large policy sets; each entry ~80 B |
| `--deny-rate-threshold` | 0 (disabled) | Set to auto-revert if deny rate spikes (e.g., 1000/s) |
| `--audit` vs `--enforce` | audit | Use enforce mode only after validating policy correctness |

## Related documents

- `docs/GUARANTEES.md` — Enforcement guarantees and TOCTOU analysis
- `docs/THREAT_MODEL.md` — Threat model and coverage boundaries
- `docs/COMPATIBILITY.md` — Kernel version compatibility
