name: Perf Regression

on:
  workflow_dispatch:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'bpf/**'
      - 'scripts/perf_*.sh'
      - 'scripts/validate_perf_artifacts.py'
      - '.github/workflows/perf.yml'
  pull_request:
    paths:
      - 'src/**'
      - 'bpf/**'
      - 'scripts/perf_*.sh'
      - 'scripts/validate_perf_artifacts.py'
      - '.github/workflows/perf.yml'
  schedule:
    - cron: "0 4 * * 0"

permissions:
  contents: read
  pull-requests: write

concurrency:
  group: perf-${{ github.ref }}
  cancel-in-progress: true

jobs:
  perf:
    runs-on: [self-hosted, perf]
    timeout-minutes: 75
    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies (if apt-get is available)
        run: |
          if command -v apt-get >/dev/null 2>&1; then
            export DEBIAN_FRONTEND=noninteractive
            pkgs="clang llvm libbpf-dev libsystemd-dev pkg-config cmake ninja-build libelf-dev zlib1g-dev git python3"
            for attempt in 1 2 3; do
              if sudo apt-get -o DPkg::Lock::Timeout=120 update && \
                 sudo apt-get -o DPkg::Lock::Timeout=120 install -y $pkgs; then
                break
              fi
              echo "apt-get attempt ${attempt} failed; retrying in 10s..."
              sleep 10
            done
            if ! pkg-config --exists libbpf || ! pkg-config --atleast-version=1.1.0 libbpf; then
              rm -rf /tmp/libbpf
              git clone --depth 1 --branch v1.4.0 https://github.com/libbpf/libbpf.git /tmp/libbpf
              make -C /tmp/libbpf/src
              sudo make -C /tmp/libbpf/src install
              echo "/usr/local/lib" | sudo tee /etc/ld.so.conf.d/libbpf.conf
              if [ -d /usr/local/lib64 ]; then
                echo "/usr/local/lib64" | sudo tee /etc/ld.so.conf.d/libbpf64.conf
              fi
              if [ -d /usr/lib64 ]; then
                echo "/usr/lib64" | sudo tee /etc/ld.so.conf.d/lib64.conf
              fi
              for dir in /usr/local/lib /usr/local/lib64; do
                if ls "$dir"/libbpf.so.* >/dev/null 2>&1; then
                  sudo ln -sf "$(ls "$dir"/libbpf.so.* | head -n1)" "$dir/libbpf.so.1"
                fi
              done
              sudo ldconfig
              echo "PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/local/lib/pkgconfig:/usr/local/lib64/pkgconfig:${PKG_CONFIG_PATH}" >> "$GITHUB_ENV"
              echo "LD_LIBRARY_PATH=/usr/lib64:/usr/local/lib:/usr/local/lib64:${LD_LIBRARY_PATH}" >> "$GITHUB_ENV"
            fi
            if ! command -v bpftool >/dev/null 2>&1; then
              sudo apt-get -o DPkg::Lock::Timeout=120 install -y linux-tools-common || true
              sudo apt-get -o DPkg::Lock::Timeout=120 install -y "linux-tools-$(uname -r)" || \
                sudo apt-get -o DPkg::Lock::Timeout=120 install -y linux-tools-generic || \
                sudo apt-get -o DPkg::Lock::Timeout=120 install -y linux-tools-gcp || true
            fi
            if ! command -v bpftool >/dev/null 2>&1; then
              echo "bpftool is required but was not installed" >&2
              exit 1
            fi
          fi

      - name: Verify environment
        run: scripts/verify_env.sh --strict

      - name: Configure
        run: |
          rm -rf build
          cmake -S . -B build -G Ninja \
            -DCMAKE_BUILD_TYPE=Release \
            -DBUILD_TESTING=OFF \
            -DSKIP_BPF_BUILD=OFF

      - name: Build
        run: cmake --build build

      - name: Prepare perf artifacts
        run: mkdir -p artifacts/perf

      - name: Perf regression (open/close microbench)
        run: |
          sudo OUT_JSON=artifacts/perf/open_compare.json \
            MAX_PCT=10 \
            ITERATIONS=200000 \
            FILE=/etc/hosts \
            scripts/perf_compare.sh

      - name: Perf workload suite (realistic file workloads)
        run: |
          sudo BIN=./build/aegisbpf \
            FILE=/etc/hosts \
            OPEN_ITERATIONS=200000 \
            CONNECT_ITERATIONS=50000 \
            READ_ITERATIONS=50000 \
            STAT_SAMPLE=400 \
            STAT_ITERATIONS=50 \
            MAX_OPEN_PCT=10 \
            MAX_CONNECT_PCT=15 \
            MAX_READ_PCT=15 \
            MAX_STAT_PCT=15 \
            FORMAT=json \
            OUT=artifacts/perf/workload_suite.json \
            scripts/perf_workload_suite.sh

      - name: Capture percentile profiles (open/connect)
        run: |
          sudo FORMAT=json OUT=artifacts/perf/open_baseline.json ITERATIONS=100000 FILE=/etc/hosts scripts/perf_open_bench.sh
          sudo WITH_AGENT=1 BIN=./build/aegisbpf FORMAT=json OUT=artifacts/perf/open_with_agent.json ITERATIONS=100000 FILE=/etc/hosts scripts/perf_open_bench.sh
          sudo FORMAT=json OUT=artifacts/perf/connect_baseline.json ITERATIONS=50000 scripts/perf_connect_bench.sh
          sudo WITH_AGENT=1 BIN=./build/aegisbpf FORMAT=json OUT=artifacts/perf/connect_with_agent.json ITERATIONS=50000 scripts/perf_connect_bench.sh

      - name: Capture perf environment
        run: |
          uname -a > artifacts/perf/kernel.txt
          cat /etc/os-release > artifacts/perf/os-release.txt || true
          lscpu > artifacts/perf/cpu.txt || true
          stat -f -c '%T' /tmp > artifacts/perf/fs.txt || true

      - name: SLO gate check
        id: slo_gate
        continue-on-error: ${{ github.event_name == 'pull_request' }}
        run: |
          OPEN_JSON=artifacts/perf/open_compare.json \
          WORKLOAD_JSON=artifacts/perf/workload_suite.json \
          OPEN_BASELINE_JSON=artifacts/perf/open_baseline.json \
          OPEN_WITH_AGENT_JSON=artifacts/perf/open_with_agent.json \
          CONNECT_BASELINE_JSON=artifacts/perf/connect_baseline.json \
          CONNECT_WITH_AGENT_JSON=artifacts/perf/connect_with_agent.json \
          REPORT_OUT=artifacts/perf/perf-slo-report.md \
          SUMMARY_OUT=artifacts/perf/perf-slo-summary.json \
          MAX_OPEN_P95_OVERHEAD=10 \
          MAX_CONNECT_P95_OVERHEAD=10 \
          scripts/perf_slo_check.sh

      - name: Validate perf artifact schema and KPI ratios
        id: schema_gate
        continue-on-error: ${{ github.event_name == 'pull_request' }}
        run: |
          python3 scripts/validate_perf_artifacts.py \
            --open-baseline artifacts/perf/open_baseline.json \
            --open-with-agent artifacts/perf/open_with_agent.json \
            --connect-baseline artifacts/perf/connect_baseline.json \
            --connect-with-agent artifacts/perf/connect_with_agent.json \
            --workload artifacts/perf/workload_suite.json \
            --report artifacts/perf/perf-evidence-report.md \
            --max-open-p95-ratio 1.05 \
            --max-connect-p95-ratio 1.05

      - name: Upload perf evidence
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-evidence
          path: artifacts/perf/
          if-no-files-found: warn

      - name: Post perf results to PR
        if: always() && github.event_name == 'pull_request' && github.event.pull_request.head.repo.full_name == github.repository
        uses: actions/github-script@v7
        env:
          SLO_OUTCOME: ${{ steps.slo_gate.outcome }}
          SCHEMA_OUTCOME: ${{ steps.schema_gate.outcome }}
        with:
          script: |
            const fs = require('fs');

            function readJsonIfExists(path, fallback) {
              try {
                return JSON.parse(fs.readFileSync(path, 'utf8'));
              } catch (e) {
                return fallback;
              }
            }

            const open = readJsonIfExists('artifacts/perf/open_compare.json', {});
            const workload = readJsonIfExists('artifacts/perf/workload_suite.json', { benchmarks: [] });
            let report = '';
            try {
              report = fs.readFileSync('artifacts/perf/perf-evidence-report.md', 'utf8');
            } catch (e) {
              report = '_report unavailable_';
            }

            const failedRows = (workload.benchmarks || [])
              .filter((row) => !row.pass)
              .map((row) => row.name);
            const status = process.env.SLO_OUTCOME === 'success' && process.env.SCHEMA_OUTCOME === 'success'
              ? ':white_check_mark:'
              : ':warning:';

            let body = '## Performance Gate (PR Advisory)\n\n';
            body += '| Check | Outcome |\n';
            body += '|---|---|\n';
            body += `| SLO gate | ${process.env.SLO_OUTCOME} |\n`;
            body += `| Artifact/KPI gate | ${process.env.SCHEMA_OUTCOME} |\n`;
            body += `| open delta_pct | ${open.delta_pct !== undefined ? `${open.delta_pct}%` : 'N/A'} |\n`;
            body += `| workload failing rows | ${failedRows.length ? failedRows.join(', ') : 'none'} |\n`;
            body += `\nOverall: ${status}\n\n`;
            body += '<details><summary>Perf Evidence Report</summary>\n\n';
            body += report;
            body += '\n</details>\n';
            body += '\n---\n`main`/scheduled runs enforce this gate strictly; PR result is advisory.\n';

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body
            });
